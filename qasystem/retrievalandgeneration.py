# RetrievalQA: LangChain class that performs question answering (QA) over documents by first retrieving relevant
# chunks and then passing them to the LLM for response generation.
# FAISS: Vector store from Facebook AI that stores document embeddings and performs similarity searches.
# Bedrock: AWS Bedrock model integration for LLM inference.
# boto3: AWS SDK to interact with AWS Bedrock runtime.
# PromptTemplate: Used to structure the prompt with custom variables.
# qasystem.ingestion: Importing previously defined data ingestion and vector store generation functions.
# BedrockEmbeddings: Embedding model from AWS Bedrock (Amazon Titan).

from langchain.chains import RetrievalQA
from langchain.vectorstores import FAISS
from langchain.llms.bedrock import Bedrock
import boto3
from langchain.prompts import PromptTemplate
from qasystem.ingestion import get_vector_store
from qasystem.ingestion import data_ingestion
from langchain_community.embeddings import BedrockEmbeddings

# Initializes the AWS Bedrock runtime client using boto3.
bedrock = boto3.client(service_name="bedrock-runtime", region_name="us-east-1")

# The Titan Embedding model (amazon.titan-embed-text-v2:0) generates vector representations of text, which 
# are essential for similarity searches.
bedrock_embeddings = BedrockEmbeddings(model_id="amazon.titan-embed-text-v2:0", client=bedrock)


# Prompt Template:
# This prompt structure follows the Human-Assistant Chatbot pattern.
# The context placeholder will be replaced with retrieved document chunks.
# The question placeholder holds the user's query.
# The response will be generated by the LLM with at least 250 words.

prompt_template = """
Human: Use the following pieces of context to provide a 
concise answer to the question at the end but usse atleast summarize with 
250 words with detailed explaantions. If you don't know the answer, 
just say that you don't know, don't try to make up an answer.
<context>
{context}
</context

Question: {question}

Assistant:"""


# Prompt object:
# Converts the raw string into a LangChain PromptTemplate.
# Defines the input variables that need to be passed during prompt execution.
PROMPT=PromptTemplate(
    template = prompt_template,input_variables=["context", "question"]
)

# Get LLM Function
def get_llama3_llm():
    llm = Bedrock(model_id="meta.llama3-8b-instruct-v1:0", client=bedrock)    
    return llm


# Response function:
# RetrievalQA initializes the QA pipeline.
# stuff chain type concatenates all retrieved documents into a single prompt.
# vectorstore_faiss.as_retriever() converts the FAISS index into a retriever object for similarity search.
# search_kwargs={"k": 3} means the top 3 most similar chunks will be retrieved.
# return_source_documents=True returns the source documents along with the answer.
# The prompt is passed using chain_type_kwargs

def get_response_llm(llm,vectorstore_faiss, query):
    qa=RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore_faiss.as_retriever(search_type="similarity", search_kwargs={"k":3}),
        return_source_documents=True,
        chain_type_kwargs={"prompt":PROMPT} 
    )
    answer=qa({"query":query}) # Executes the chain by passing the query.
    return answer["result"]
    


if __name__=='__main__':
    # Loads the FAISS index from the local directory where embeddings were previously stored.
    # The allow_dangerous_deserialization=True flag bypasses serialization safety checks (use with caution).
    faiss_index = FAISS.load_local("../faiss_index", bedrock_embeddings, allow_dangerous_deserialization=True)
    
    query = "What is RAG token?"
    llm = get_llama3_llm()
    print(get_response_llm(llm, faiss_index,query))
    